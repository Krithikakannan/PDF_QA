# ğŸ“„ PDF Question Answering System (Offline)

This project is a local, offline PDF Question Answering system built using Retrieval-Augmented Generation (RAG). Users can upload a PDF, ask questions, and receive answers strictly grounded in the document content.

---

## ğŸš€ Features

- Upload and process PDF files
- Ask natural language questions
- Accurate answers using document context
- Fully offline (no internet required)
- Uses local LLM via Ollama (LLaMA 3)
- Web UI built with Streamlit
- Persistent vector storage using ChromaDB

---

## ğŸ› ï¸ Tech Stack

- Python
- PyMuPDF (PDF extraction)
- SentenceTransformers (Embeddings)
- ChromaDB (Vector database)
- Ollama + LLaMA 3 (Local LLM)
- Streamlit (Web UI)

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ extract_pdf.py # Extracts PDF and stores embeddings
â”œâ”€â”€ ask_pdf.py # CLI-based question answering
â”œâ”€â”€ app.py # Streamlit web application
â”œâ”€â”€ chroma_db/ # Persistent vector database


---

## âš™ï¸ Setup Instructions

### 1. Install Dependencies
```bash
pip install streamlit chromadb sentence-transformers pymupdf
2. Install Ollama & LLaMA 3
ollama pull llama3

3. Run Streamlit App
streamlit run app.py

ğŸ§  How It Works

Extracts text from PDF

Splits text into overlapping chunks

Generates embeddings

Stores embeddings in ChromaDB

Retrieves relevant chunks based on question

Sends context to LLaMA 3

Displays answer

ğŸ“Œ Notes

The system runs completely offline

Answers are generated strictly from PDF content

No data is sent to external servers

ğŸ”® Future Enhancements

Chat history

Voice input

Multi-document support

User authentication

Mobile-friendly UI
